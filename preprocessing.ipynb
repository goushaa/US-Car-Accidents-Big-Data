{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2798876,"sourceType":"datasetVersion","datasetId":1709663},{"sourceId":5793796,"sourceType":"datasetVersion","datasetId":199387},{"sourceId":11578323,"sourceType":"datasetVersion","datasetId":7259608}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# US Accidents Dataset Preprocessing\n\nIn this notebook, we demonstrate our preprocessing workflow for the US Accidents dataset using PySpark. We efficiently clean, transform, and prepare the data for analysis. By the end of this notebook, we will have generated a new, cleaned CSV file that is ready for analysis and modeling in another notebook.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import count, when, isnull, udf, col, concat_ws, trim, upper\nfrom pyspark.ml.feature import QuantileDiscretizer, VectorAssembler\nfrom pyspark.ml.stat import Correlation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:00:54.230289Z","iopub.execute_input":"2025-04-27T23:00:54.230691Z","iopub.status.idle":"2025-04-27T23:00:55.113364Z","shell.execute_reply.started":"2025-04-27T23:00:54.230659Z","shell.execute_reply":"2025-04-27T23:00:55.112191Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Data Loading\n\nIn this step, we load the US Accidents dataset into a PySpark DataFrame. Initially, we have loaded **7,728,394 rows** and **46 columns**.","metadata":{}},{"cell_type":"code","source":"# Initialize Spark session with increased memory (Kaggle Max is 30GiB)\nspark = SparkSession.builder.appName(\"project\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"28g\") \\\n    .config(\"spark.executor.memory\", \"28g\") \\\n    .getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\ndata_path = \"/kaggle/input/us-accidents/US_Accidents_March23.csv\"\n\n# Load the CSV data into a PySpark DataFrame\ndf = spark.read.csv(data_path, header=True, inferSchema=True).cache()\n\nprint(f\"Loaded {df.count()} rows and {len(df.columns)} columns.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:00:55.115437Z","iopub.execute_input":"2025-04-27T23:00:55.115979Z","iopub.status.idle":"2025-04-27T23:03:41.855768Z","shell.execute_reply.started":"2025-04-27T23:00:55.115948Z","shell.execute_reply":"2025-04-27T23:03:41.853754Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/27 23:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[Stage 3:==============================================>                              (14 + 4) / 23]\r","output_type":"stream"},{"name":"stdout","text":"Loaded 7728394 rows and 46 columns.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Detecting Missing Values\n\nWe analyze the dataset for missing (NULL) values by counting and calculating the percentage of NULLs in each column. We found out that **End_Lat**, **End_Lng**, **Precipitation(in)**, and **Wind_Chill(F)** have a high proportion of missing values. Since these columns contain a significant amount of missing data, we will discard them in the next steps.","metadata":{}},{"cell_type":"code","source":"# Count total rows in the DataFrame\nrow_count = df.count()\n\n# Calculate NULL count for each column\nnull_count_by_column = df.select([\n    count(when(isnull(column_name), column_name)).alias(column_name) for column_name in df.columns\n]).collect()[0].asDict()\n\n# Calculate NULL percentage for each column\nnull_percentage_by_column = {\n    column_name: (null_count / row_count * 100) for column_name, null_count in null_count_by_column.items()\n}\n\n# Sort columns by NULL count in descending order\nnull_stats_sorted = sorted(null_count_by_column.items(), key=lambda item: item[1], reverse=True)\n\nprint(\"=== NULL Count and Percentage by Column ===\")\nfor column_name, null_count in null_stats_sorted:\n    null_percentage = null_percentage_by_column[column_name]\n    print(f\"{column_name}: {null_count} ({null_percentage:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:03:41.856819Z","iopub.execute_input":"2025-04-27T23:03:41.857163Z","iopub.status.idle":"2025-04-27T23:03:56.919659Z","shell.execute_reply.started":"2025-04-27T23:03:41.857130Z","shell.execute_reply":"2025-04-27T23:03:56.918851Z"}},"outputs":[{"name":"stderr","text":"[Stage 9:=========================================================================>   (22 + 1) / 23]\r","output_type":"stream"},{"name":"stdout","text":"=== NULL Count and Percentage by Column ===\nEnd_Lat: 3402762 (44.03%)\nEnd_Lng: 3402762 (44.03%)\nPrecipitation(in): 2203586 (28.51%)\nWind_Chill(F): 1999019 (25.87%)\nWind_Speed(mph): 571233 (7.39%)\nVisibility(mi): 177098 (2.29%)\nWind_Direction: 175206 (2.27%)\nHumidity(%): 174144 (2.25%)\nWeather_Condition: 173459 (2.24%)\nTemperature(F): 163853 (2.12%)\nPressure(in): 140679 (1.82%)\nWeather_Timestamp: 120228 (1.56%)\nSunrise_Sunset: 23246 (0.30%)\nCivil_Twilight: 23246 (0.30%)\nNautical_Twilight: 23246 (0.30%)\nAstronomical_Twilight: 23246 (0.30%)\nAirport_Code: 22635 (0.29%)\nStreet: 10869 (0.14%)\nTimezone: 7808 (0.10%)\nZipcode: 1915 (0.02%)\nCity: 253 (0.00%)\nDescription: 5 (0.00%)\nID: 0 (0.00%)\nSource: 0 (0.00%)\nSeverity: 0 (0.00%)\nStart_Time: 0 (0.00%)\nEnd_Time: 0 (0.00%)\nStart_Lat: 0 (0.00%)\nStart_Lng: 0 (0.00%)\nDistance(mi): 0 (0.00%)\nCounty: 0 (0.00%)\nState: 0 (0.00%)\nCountry: 0 (0.00%)\nAmenity: 0 (0.00%)\nBump: 0 (0.00%)\nCrossing: 0 (0.00%)\nGive_Way: 0 (0.00%)\nJunction: 0 (0.00%)\nNo_Exit: 0 (0.00%)\nRailway: 0 (0.00%)\nRoundabout: 0 (0.00%)\nStation: 0 (0.00%)\nStop: 0 (0.00%)\nTraffic_Calming: 0 (0.00%)\nTraffic_Signal: 0 (0.00%)\nTurning_Loop: 0 (0.00%)\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Handling Missing Data\n\nTo ensure data quality, we first dropped columns with more than 25% missing valuesâ€”specifically, **End_Lat**, **End_Lng**, **Wind_Chill(F)**, and **Precipitation(in)**. Next, we dropped all remaining rows containing any NULLs, as the percentage of missing values in the remaining columns was low. This left us with a fully clean dataset of **7,051,556 rows** and **42 columns**.","metadata":{}},{"cell_type":"code","source":"# Drop columns with more than 25% NULLs\ncolumns_to_drop = [col for col, pct in null_percentage_by_column.items() if pct > 25]\ndf = df.drop(*columns_to_drop)\nprint(f\"Dropped columns with >25% NULLs: {columns_to_drop}\")\n\n# Recompute NULL counts and percentages after dropping columns\nrow_count = df.count()\nnull_count_by_column = df.select([\n    count(when(isnull(column_name), column_name)).alias(column_name) for column_name in df.columns\n]).collect()[0].asDict()\nnull_percentage_by_column = {\n    column_name: (null_count / row_count * 100) for column_name, null_count in null_count_by_column.items()\n}\nnull_stats_sorted = sorted(null_count_by_column.items(), key=lambda item: item[1], reverse=True)\n\nprint(\"=== NULL Count and Percentage by Column (After Drop) ===\")\nfor column_name, null_count in null_stats_sorted:\n    null_percentage = null_percentage_by_column[column_name]\n    print(f\"{column_name}: {null_count} ({null_percentage:.2f}%)\")\n\n# Drop all rows with any remaining NULLs (row-wise drop)\n# This step is safe as the remaining NULL percentages are very low.\ndf = df.na.drop()\nprint(\"\\nDropped all rows with any NULLs row-wise.\")\n\n# Confirm dataset is fully clean\nrow_count = df.count()\ncol_count = len(df.columns)\nnull_count_by_column = df.select([\n    count(when(isnull(column_name), column_name)).alias(column_name) for column_name in df.columns\n]).collect()[0].asDict()\n\nprint(f\"\\nAfter all drops, row count: {row_count}, column count: {col_count}\")\nprint(\"\\n=== NULL Count by Column (Should all be 0) ===\")\nfor column_name, null_count in null_count_by_column.items():\n    print(f\"{column_name}: {null_count}\")\n\nprint(\"\\nAll NULLs have been dropped.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:03:56.923430Z","iopub.execute_input":"2025-04-27T23:03:56.925131Z","iopub.status.idle":"2025-04-27T23:04:33.923366Z","shell.execute_reply.started":"2025-04-27T23:03:56.925093Z","shell.execute_reply":"2025-04-27T23:04:33.921090Z"}},"outputs":[{"name":"stdout","text":"Dropped columns with >25% NULLs: ['End_Lat', 'End_Lng', 'Wind_Chill(F)', 'Precipitation(in)']\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"=== NULL Count and Percentage by Column (After Drop) ===\nWind_Speed(mph): 571233 (7.39%)\nVisibility(mi): 177098 (2.29%)\nWind_Direction: 175206 (2.27%)\nHumidity(%): 174144 (2.25%)\nWeather_Condition: 173459 (2.24%)\nTemperature(F): 163853 (2.12%)\nPressure(in): 140679 (1.82%)\nWeather_Timestamp: 120228 (1.56%)\nSunrise_Sunset: 23246 (0.30%)\nCivil_Twilight: 23246 (0.30%)\nNautical_Twilight: 23246 (0.30%)\nAstronomical_Twilight: 23246 (0.30%)\nAirport_Code: 22635 (0.29%)\nStreet: 10869 (0.14%)\nTimezone: 7808 (0.10%)\nZipcode: 1915 (0.02%)\nCity: 253 (0.00%)\nDescription: 5 (0.00%)\nID: 0 (0.00%)\nSource: 0 (0.00%)\nSeverity: 0 (0.00%)\nStart_Time: 0 (0.00%)\nEnd_Time: 0 (0.00%)\nStart_Lat: 0 (0.00%)\nStart_Lng: 0 (0.00%)\nDistance(mi): 0 (0.00%)\nCounty: 0 (0.00%)\nState: 0 (0.00%)\nCountry: 0 (0.00%)\nAmenity: 0 (0.00%)\nBump: 0 (0.00%)\nCrossing: 0 (0.00%)\nGive_Way: 0 (0.00%)\nJunction: 0 (0.00%)\nNo_Exit: 0 (0.00%)\nRailway: 0 (0.00%)\nRoundabout: 0 (0.00%)\nStation: 0 (0.00%)\nStop: 0 (0.00%)\nTraffic_Calming: 0 (0.00%)\nTraffic_Signal: 0 (0.00%)\nTurning_Loop: 0 (0.00%)\n\nDropped all rows with any NULLs row-wise.\n","output_type":"stream"},{"name":"stderr","text":"[Stage 21:=====================================================================>      (21 + 2) / 23]\r","output_type":"stream"},{"name":"stdout","text":"\nAfter all drops, row count: 7051556, column count: 42\n\n=== NULL Count by Column (Should all be 0) ===\nID: 0\nSource: 0\nSeverity: 0\nStart_Time: 0\nEnd_Time: 0\nStart_Lat: 0\nStart_Lng: 0\nDistance(mi): 0\nDescription: 0\nStreet: 0\nCity: 0\nCounty: 0\nState: 0\nZipcode: 0\nCountry: 0\nTimezone: 0\nAirport_Code: 0\nWeather_Timestamp: 0\nTemperature(F): 0\nHumidity(%): 0\nPressure(in): 0\nVisibility(mi): 0\nWind_Direction: 0\nWind_Speed(mph): 0\nWeather_Condition: 0\nAmenity: 0\nBump: 0\nCrossing: 0\nGive_Way: 0\nJunction: 0\nNo_Exit: 0\nRailway: 0\nRoundabout: 0\nStation: 0\nStop: 0\nTraffic_Calming: 0\nTraffic_Signal: 0\nTurning_Loop: 0\nSunrise_Sunset: 0\nCivil_Twilight: 0\nNautical_Twilight: 0\nAstronomical_Twilight: 0\n\nAll NULLs have been dropped.\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Dropping Irrelevant Columns\n\nBefore dropping, we inspected the unique values in the **Source**, **Country**, and **Turning_Loop** columns. We then removed the following columns as they do not provide analytical value:\n- **ID**: Unique identifier, not useful for analysis.\n- **Country**: Contains only one value ('US').\n- **Source**: Contains only a few unique values (Source1, Source2, Source3), not informative.\n- **Turning_Loop**: Contains only one value (False).\n\nAfter this step, our dataset contains 38 relevant columns.","metadata":{}},{"cell_type":"code","source":"# Show unique values for 'Source', 'Country', and 'Turning_Loop' before dropping\nif \"Source\" in df.columns:\n    print(\"Unique values in 'Source':\")\n    df.select(\"Source\").distinct().show(truncate=False)\n\nif \"Country\" in df.columns:\n    print(\"Unique values in 'Country':\")\n    df.select(\"Country\").distinct().show(truncate=False)\n\nif \"Turning_Loop\" in df.columns:\n    print(\"Unique values in 'Turning_Loop':\")\n    df.select(\"Turning_Loop\").distinct().show(truncate=False)\n\n# Identify columns to drop\ncols_to_drop = []\nif \"ID\" in df.columns:\n    cols_to_drop.append(\"ID\")\nif \"Country\" in df.columns:\n    cols_to_drop.append(\"Country\")\nif \"Source\" in df.columns:\n    cols_to_drop.append(\"Source\")\nif \"Turning_Loop\" in df.columns:\n    cols_to_drop.append(\"Turning_Loop\")\n\n# Drop the identified columns\ndf = df.drop(*cols_to_drop)\nprint(f\"Dropped columns: {cols_to_drop}\")\n\n# Document reasoning\nprint(\"\"\"\nReasoning:\n- 'ID' is a unique identifier for each row and does not add analytical value.\n- 'Country' has only one value ('US'), so it does not help distinguish data.\n- 'Source' has only 3 values and does not add analytical value.\n- 'Turning_Loop' has only one value and does not add analytical value.\n\"\"\")\n\nprint(f\"\\nAfter all drops, {len(df.columns)} columns remain: {', '.join(df.columns)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:04:33.926064Z","iopub.execute_input":"2025-04-27T23:04:33.926832Z","iopub.status.idle":"2025-04-27T23:05:02.315151Z","shell.execute_reply.started":"2025-04-27T23:04:33.926788Z","shell.execute_reply":"2025-04-27T23:05:02.314186Z"}},"outputs":[{"name":"stdout","text":"Unique values in 'Source':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+-------+\n|Source |\n+-------+\n|Source3|\n|Source2|\n|Source1|\n+-------+\n\nUnique values in 'Country':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+-------+\n|Country|\n+-------+\n|US     |\n+-------+\n\nUnique values in 'Turning_Loop':\n","output_type":"stream"},{"name":"stderr","text":"[Stage 30:========================================================================>   (22 + 1) / 23]\r","output_type":"stream"},{"name":"stdout","text":"+------------+\n|Turning_Loop|\n+------------+\n|false       |\n+------------+\n\nDropped columns: ['ID', 'Country', 'Source', 'Turning_Loop']\n\nReasoning:\n- 'ID' is a unique identifier for each row and does not add analytical value.\n- 'Country' has only one value ('US'), so it does not help distinguish data.\n- 'Source' has only 3 values and does not add analytical value.\n- 'Turning_Loop' has only one value and does not add analytical value.\n\n\nAfter all drops, 38 columns remain: Severity, Start_Time, End_Time, Start_Lat, Start_Lng, Distance(mi), Description, Street, City, County, State, Zipcode, Timezone, Airport_Code, Weather_Timestamp, Temperature(F), Humidity(%), Pressure(in), Visibility(mi), Wind_Direction, Wind_Speed(mph), Weather_Condition, Amenity, Bump, Crossing, Give_Way, Junction, No_Exit, Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic_Signal, Sunrise_Sunset, Civil_Twilight, Nautical_Twilight, Astronomical_Twilight\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Checking for Redundant or Highly Correlated Columns\n\nWe now inspect the dataset for redundant or highly correlated columns. We visually examine groups of columns with similar meanings and check for high correlation among continuous variables to see if we have to remove any columns in the next cell.\n\n**Findings:**\n- **Timestamps:** Both `Start_Time` and `End_Time` are present and provide complementary information about accident duration.\n- **Location:** Columns like `Street`, `City`, `County`, `State`, and `Zipcode` offer different levels of location granularity, but each may be useful for different types of analysis.\n- **Twilight Columns:** `Sunrise_Sunset`, `Civil_Twilight`, `Nautical_Twilight`, and `Astronomical_Twilight` all have only two unique values (`Day`/`Night`), suggesting potential redundancy.\n- **Continuous Variables:** The correlation matrix shows no extremely high correlations (absolute value close to 1) between the continuous variables, so we do not need to drop any due to multicollinearity.\n\nBased on these results, we will consider dropping redundant twilight columns in the next step.","metadata":{}},{"cell_type":"code","source":"print(\"=== Checking for Redundant or Highly Correlated Columns ===\")\n\n# Inspect columns with similar meanings\npotential_redundant_groups = [\n    [\"Start_Time\", \"End_Time\"],  # Both are timestamps\n    [\"Street\", \"City\", \"County\", \"State\", \"Zipcode\"],  # Location granularity\n    [\"Sunrise_Sunset\", \"Civil_Twilight\", \"Nautical_Twilight\", \"Astronomical_Twilight\"],  # Twilight/time-of-day\n]\n\nfor group in potential_redundant_groups:\n    present = [c for c in group if c in df.columns]\n    if len(present) > 1:\n        print(f\"\\nSample values for columns: {present}\")\n        df.select(*present).show(5, truncate=False)\n\n# Check correlation between continuous variables\ncontinuous_cols = [\n    \"Distance(mi)\", \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\"\n]\npresent_continuous = [c for c in continuous_cols if c in df.columns]\n\nif len(present_continuous) > 1:\n    assembler = VectorAssembler(inputCols=present_continuous, outputCol=\"features_corr\")\n    df_corr = assembler.transform(df.select(*present_continuous).dropna())\n    corr_matrix = Correlation.corr(df_corr, \"features_corr\").head()[0].toArray()\n    print(\"\\nCorrelation matrix for continuous variables:\")\n    print(f\"Columns: {present_continuous}\")\n    print(corr_matrix)\n\n# Show unique values for twilight columns to check for redundancy\ntwilight_cols = [\"Sunrise_Sunset\", \"Civil_Twilight\", \"Nautical_Twilight\", \"Astronomical_Twilight\"]\nfor col_name in twilight_cols:\n    if col_name in df.columns:\n        print(f\"\\nUnique values in '{col_name}':\")\n        df.select(col_name).distinct().show(truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:05:02.316328Z","iopub.execute_input":"2025-04-27T23:05:02.316660Z","iopub.status.idle":"2025-04-27T23:06:14.259441Z","shell.execute_reply.started":"2025-04-27T23:05:02.316632Z","shell.execute_reply":"2025-04-27T23:06:14.258368Z"}},"outputs":[{"name":"stdout","text":"=== Checking for Redundant or Highly Correlated Columns ===\n\nSample values for columns: ['Start_Time', 'End_Time']\n+-------------------+-------------------+\n|Start_Time         |End_Time           |\n+-------------------+-------------------+\n|2016-02-08 06:49:27|2016-02-08 07:19:27|\n|2016-02-08 07:23:34|2016-02-08 07:53:34|\n|2016-02-08 07:39:07|2016-02-08 08:09:07|\n|2016-02-08 07:44:26|2016-02-08 08:14:26|\n|2016-02-08 07:59:35|2016-02-08 08:29:35|\n+-------------------+-------------------+\nonly showing top 5 rows\n\n\nSample values for columns: ['Street', 'City', 'County', 'State', 'Zipcode']\n+-------------------------+------------+----------+-----+----------+\n|Street                   |City        |County    |State|Zipcode   |\n+-------------------------+------------+----------+-----+----------+\n|State Route 32           |Williamsburg|Clermont  |OH   |45176     |\n|I-75 S                   |Dayton      |Montgomery|OH   |45417     |\n|Miamisburg Centerville Rd|Dayton      |Montgomery|OH   |45459     |\n|Westerville Rd           |Westerville |Franklin  |OH   |43081     |\n|N Woodward Ave           |Dayton      |Montgomery|OH   |45417-2476|\n+-------------------------+------------+----------+-----+----------+\nonly showing top 5 rows\n\n\nSample values for columns: ['Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n+--------------+--------------+-----------------+---------------------+\n|Sunrise_Sunset|Civil_Twilight|Nautical_Twilight|Astronomical_Twilight|\n+--------------+--------------+-----------------+---------------------+\n|Night         |Night         |Day              |Day                  |\n|Night         |Day           |Day              |Day                  |\n|Day           |Day           |Day              |Day                  |\n|Day           |Day           |Day              |Day                  |\n|Day           |Day           |Day              |Day                  |\n+--------------+--------------+-----------------+---------------------+\nonly showing top 5 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"\nCorrelation matrix for continuous variables:\nColumns: ['Distance(mi)', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']\n[[ 1.         -0.05598764  0.01140223 -0.09040334 -0.0395247   0.00874711]\n [-0.05598764  1.         -0.33053564  0.12005678  0.22400662  0.03458547]\n [ 0.01140223 -0.33053564  1.          0.10986754 -0.38682745 -0.17264725]\n [-0.09040334  0.12005678  0.10986754  1.          0.04117808 -0.0222837 ]\n [-0.0395247   0.22400662 -0.38682745  0.04117808  1.          0.01454486]\n [ 0.00874711  0.03458547 -0.17264725 -0.0222837   0.01454486  1.        ]]\n\nUnique values in 'Sunrise_Sunset':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+--------------+\n|Sunrise_Sunset|\n+--------------+\n|Night         |\n|Day           |\n+--------------+\n\n\nUnique values in 'Civil_Twilight':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+--------------+\n|Civil_Twilight|\n+--------------+\n|Night         |\n|Day           |\n+--------------+\n\n\nUnique values in 'Nautical_Twilight':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+-----------------+\n|Nautical_Twilight|\n+-----------------+\n|Night            |\n|Day              |\n+-----------------+\n\n\nUnique values in 'Astronomical_Twilight':\n","output_type":"stream"},{"name":"stderr","text":"[Stage 51:==================================================================>         (20 + 3) / 23]\r","output_type":"stream"},{"name":"stdout","text":"+---------------------+\n|Astronomical_Twilight|\n+---------------------+\n|Night                |\n|Day                  |\n+---------------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Dropping Redundant Twilight Columns\n\nBased on our previous inspection, we drop the redundant twilight columnsâ€”**Civil_Twilight**, **Nautical_Twilight**, and **Astronomical_Twilight**â€”and keep only **Sunrise_Sunset** for simplicity. We retain **End_Time** for accident duration analysis and **Street** for potential rural/urban analysis in future notebooks.","metadata":{}},{"cell_type":"code","source":"cols_to_drop = []\n\n# Drop redundant twilight columns (keep only 'Sunrise_Sunset')\ntwilight_cols_to_drop = [\"Civil_Twilight\", \"Nautical_Twilight\", \"Astronomical_Twilight\"]\ncols_to_drop += [col for col in twilight_cols_to_drop if col in df.columns]\n\n# Keep 'End_Time' to allow for accident duration analysis\n# Keep 'Street' to combine with State for Rural/Urban Analysis\n\ndf = df.drop(*cols_to_drop)\nprint(f\"Dropped columns due to redundancy: {cols_to_drop}\")\n\nprint(\"\"\"\nReasoning:\n- Kept 'Street' for potential rural/urban analysis.\n- Kept 'End_Time' to allow for analysis of accident durations.\n- Dropped twilight columns except 'Sunrise_Sunset' as they are redundant.\n\"\"\")\n\nprint(f\"\\nAfter this step, {len(df.columns)} columns remain: {', '.join(df.columns)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:06:14.260232Z","iopub.execute_input":"2025-04-27T23:06:14.261251Z","iopub.status.idle":"2025-04-27T23:06:14.294323Z","shell.execute_reply.started":"2025-04-27T23:06:14.261217Z","shell.execute_reply":"2025-04-27T23:06:14.292846Z"}},"outputs":[{"name":"stdout","text":"Dropped columns due to redundancy: ['Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n\nReasoning:\n- Kept 'Street' for potential rural/urban analysis.\n- Kept 'End_Time' to allow for analysis of accident durations.\n- Dropped twilight columns except 'Sunrise_Sunset' as they are redundant.\n\n\nAfter this step, 35 columns remain: Severity, Start_Time, End_Time, Start_Lat, Start_Lng, Distance(mi), Description, Street, City, County, State, Zipcode, Timezone, Airport_Code, Weather_Timestamp, Temperature(F), Humidity(%), Pressure(in), Visibility(mi), Wind_Direction, Wind_Speed(mph), Weather_Condition, Amenity, Bump, Crossing, Give_Way, Junction, No_Exit, Railway, Roundabout, Station, Stop, Traffic_Calming, Traffic_Signal, Sunrise_Sunset\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Outlier Detection and Removal\n\nWe detect outliers in the continuous variables by calculating the 2nd and 98th percentiles for each. We then remove any rows where values fall outside this range. \n\nWe experimented with different thresholds:\n- Using the 1st and 99th percentiles dropped almost no rows (stayed 7 million).\n- Using the 2.5th and 97.5th percentiles reduced the dataset to about 4 million rows.\n- The 2nd and 98th percentiles provided a good balance, resulting in a cleaned dataset of **6,141,325 rows**.","metadata":{}},{"cell_type":"code","source":"print(\"=== Outlier Detection for Continuous Variables (2nd/98th Percentiles) ===\")\n\ncontinuous_cols = [\n    \"Distance(mi)\", \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\"\n]\npresent_continuous = [c for c in continuous_cols if c in df.columns]\n\noutlier_bounds_2_98 = {}\n\nfor c in present_continuous:\n    # Calculate 2nd and 98th percentiles for each column\n    q2, q98 = df.approxQuantile(c, [0.02, 0.98], 0.01)\n    outlier_bounds_2_98[c] = (q2, q98)\n    print(f\"{c}: 2nd percentile = {q2}, 98th percentile = {q98}\")\n\nprint(\"\\nSuggested action: Remove values outside [2nd, 98th] percentiles for each variable.\")\n\n# Remove outliers outside [2nd, 98th] percentiles\nfor c in present_continuous:\n    lower, upper = outlier_bounds_2_98[c]\n    df = df.filter((col(c) >= lower) & (col(c) <= upper))\n\nprint(\"\\nOutliers outside [2nd, 98th] percentiles have been removed for continuous variables.\")\nprint(f\"After outlier removal, row count: {df.count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:06:14.296170Z","iopub.execute_input":"2025-04-27T23:06:14.296662Z","iopub.status.idle":"2025-04-27T23:07:26.325924Z","shell.execute_reply.started":"2025-04-27T23:06:14.296624Z","shell.execute_reply":"2025-04-27T23:07:26.323388Z"}},"outputs":[{"name":"stdout","text":"=== Outlier Detection for Continuous Variables (2nd/98th Percentiles) ===\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Distance(mi): 2nd percentile = 0.0, 98th percentile = 4.496\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Temperature(F): 2nd percentile = 17.0, 98th percentile = 93.0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Humidity(%): 2nd percentile = 15.0, 98th percentile = 100.0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Pressure(in): 2nd percentile = 25.27, 98th percentile = 30.35\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Visibility(mi): 2nd percentile = 1.0, 98th percentile = 10.0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Wind_Speed(mph): 2nd percentile = 0.0, 98th percentile = 20.0\n\nSuggested action: Remove values outside [2nd, 98th] percentiles for each variable.\n\nOutliers outside [2nd, 98th] percentiles have been removed for continuous variables.\n","output_type":"stream"},{"name":"stderr","text":"[Stage 66:========================================================================>   (22 + 1) / 23]\r","output_type":"stream"},{"name":"stdout","text":"After outlier removal, row count: 6141325\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Equal-Width Binning for Continuous Variables\n\nWe discretize the main continuous variables using equal-width binning. For each variable, we create a binned version (with the original column name) and retain the original as a `_cont` column. For temperature, we convert values from Fahrenheit to Celsius before binning. Now, each continuous variable has its corresponding discretized version in the dataset.","metadata":{}},{"cell_type":"code","source":"# Define binning settings for each continuous variable\nequal_width_bins = {\n    \"Distance(mi)\": {\"min\": 0.0, \"max\": 4.5, \"bins\": 3},\n    \"Temperature(F)\": {\"min\": 17.0, \"max\": 93.0, \"bins\": 4},  # Converted to Celsius\n    \"Humidity(%)\": {\"min\": 17.0, \"max\": 100.0, \"bins\": 4},\n    \"Pressure(in)\": {\"min\": 25.46, \"max\": 30.35, \"bins\": 3},\n    \"Visibility(mi)\": {\"min\": 1.0, \"max\": 10.0, \"bins\": 3},\n    \"Wind_Speed(mph)\": {\"min\": 0.0, \"max\": 20.0, \"bins\": 4}\n}\n\nfor col_name, settings in equal_width_bins.items():\n    if col_name in df.columns:\n        # Rename original to _cont\n        if f\"{col_name}_cont\" not in df.columns:\n            df = df.withColumnRenamed(col_name, f\"{col_name}_cont\")\n        min_v, max_v, bins = settings[\"min\"], settings[\"max\"], settings[\"bins\"]\n        width = (max_v - min_v) / bins\n        if col_name == \"Temperature(F)\":\n            # Convert to Celsius\n            c_min = (min_v - 32) / 1.8\n            c_max = (max_v - 32) / 1.8\n            c_width = (c_max - c_min) / bins\n            df = df.withColumn(\"Temperature(C)_cont\", (col(\"Temperature(F)_cont\") - 32) / 1.8)\n            # Bin in Celsius\n            bin_expr = (\n                when(col(\"Temperature(C)_cont\") < c_min + c_width, f\"{c_min:.2f}â€“{c_min + c_width:.2f}\")\n                .when(col(\"Temperature(C)_cont\") < c_min + 2 * c_width, f\"{c_min + c_width:.2f}â€“{c_min + 2 * c_width:.2f}\")\n                .when(col(\"Temperature(C)_cont\") < c_min + 3 * c_width, f\"{c_min + 2 * c_width:.2f}â€“{c_min + 3 * c_width:.2f}\")\n                .otherwise(f\"{c_min + 3 * c_width:.2f}â€“{c_max:.2f}\")\n            )\n            df = df.withColumn(\"Temperature(C)\", bin_expr)\n        else:\n            # Bin the rest\n            if bins == 3:\n                bin_expr = (\n                    when(col(f\"{col_name}_cont\") < min_v + width, f\"{min_v:.2f}â€“{min_v + width:.2f}\")\n                    .when(col(f\"{col_name}_cont\") < min_v + 2 * width, f\"{min_v + width:.2f}â€“{min_v + 2 * width:.2f}\")\n                    .otherwise(f\"{min_v + 2 * width:.2f}â€“{max_v:.2f}\")\n                )\n            elif bins == 4:\n                bin_expr = (\n                    when(col(f\"{col_name}_cont\") < min_v + width, f\"{min_v:.2f}â€“{min_v + width:.2f}\")\n                    .when(col(f\"{col_name}_cont\") < min_v + 2 * width, f\"{min_v + width:.2f}â€“{min_v + 2 * width:.2f}\")\n                    .when(col(f\"{col_name}_cont\") < min_v + 3 * width, f\"{min_v + 2 * width:.2f}â€“{min_v + 3 * width:.2f}\")\n                    .otherwise(f\"{min_v + 3 * width:.2f}â€“{max_v:.2f}\")\n                )\n            df = df.withColumn(col_name, bin_expr)\n\n# Remove Fahrenheit columns after conversion\nif \"Temperature(F)_cont\" in df.columns:\n    df = df.drop(\"Temperature(F)_cont\")\nif \"Temperature(F)\" in df.columns:\n    df = df.drop(\"Temperature(F)\")\n\n# Show a sample of the new discretized columns to verify binning\ndiscretized_cols = [\"Distance(mi)\", \"Temperature(C)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\"]\nfor col_name in discretized_cols:\n    if col_name in df.columns:\n        print(f\"\\nValue counts for '{col_name}':\")\n        df.groupBy(col_name).count().orderBy(col_name).show(truncate=False)\n\n# Show a few rows to confirm columns exist and are populated\nprint(\"\\nSample rows with discretized columns:\")\ndf.select(discretized_cols).show(5, truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:07:26.327254Z","iopub.execute_input":"2025-04-27T23:07:26.328261Z","iopub.status.idle":"2025-04-27T23:08:24.676704Z","shell.execute_reply.started":"2025-04-27T23:07:26.328224Z","shell.execute_reply":"2025-04-27T23:08:24.675875Z"}},"outputs":[{"name":"stdout","text":"\nValue counts for 'Distance(mi)':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+------------+-------+\n|Distance(mi)|count  |\n+------------+-------+\n|0.00â€“1.50   |5674110|\n|1.50â€“3.00   |343977 |\n|3.00â€“4.50   |123238 |\n+------------+-------+\n\n\nValue counts for 'Temperature(C)':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+--------------+-------+\n|Temperature(C)|count  |\n+--------------+-------+\n|-8.33â€“2.22    |472710 |\n|12.78â€“23.33   |2390579|\n|2.22â€“12.78    |1410634|\n|23.33â€“33.89   |1867402|\n+--------------+-------+\n\n\nValue counts for 'Humidity(%)':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+------------+-------+\n|Humidity(%) |count  |\n+------------+-------+\n|17.00â€“37.75 |729271 |\n|37.75â€“58.50 |1532462|\n|58.50â€“79.25 |1923952|\n|79.25â€“100.00|1955640|\n+------------+-------+\n\n\nValue counts for 'Pressure(in)':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+------------+-------+\n|Pressure(in)|count  |\n+------------+-------+\n|25.46â€“27.09 |117139 |\n|27.09â€“28.72 |277357 |\n|28.72â€“30.35 |5746829|\n+------------+-------+\n\n\nValue counts for 'Visibility(mi)':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+--------------+-------+\n|Visibility(mi)|count  |\n+--------------+-------+\n|1.00â€“4.00     |309867 |\n|4.00â€“7.00     |323758 |\n|7.00â€“10.00    |5507700|\n+--------------+-------+\n\n\nValue counts for 'Wind_Speed(mph)':\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+---------------+-------+\n|Wind_Speed(mph)|count  |\n+---------------+-------+\n|0.00â€“5.00      |1672798|\n|10.00â€“15.00    |1229628|\n|15.00â€“20.00    |498514 |\n|5.00â€“10.00     |2740385|\n+---------------+-------+\n\n\nSample rows with discretized columns:\n+------------+--------------+------------+------------+--------------+---------------+\n|Distance(mi)|Temperature(C)|Humidity(%) |Pressure(in)|Visibility(mi)|Wind_Speed(mph)|\n+------------+--------------+------------+------------+--------------+---------------+\n|0.00â€“1.50   |2.22â€“12.78    |79.25â€“100.00|28.72â€“30.35 |7.00â€“10.00    |0.00â€“5.00      |\n|0.00â€“1.50   |-8.33â€“2.22    |79.25â€“100.00|28.72â€“30.35 |7.00â€“10.00    |0.00â€“5.00      |\n|0.00â€“1.50   |2.22â€“12.78    |79.25â€“100.00|28.72â€“30.35 |4.00â€“7.00     |0.00â€“5.00      |\n|0.00â€“1.50   |2.22â€“12.78    |79.25â€“100.00|28.72â€“30.35 |7.00â€“10.00    |0.00â€“5.00      |\n|0.00â€“1.50   |-8.33â€“2.22    |79.25â€“100.00|28.72â€“30.35 |7.00â€“10.00    |0.00â€“5.00      |\n+------------+--------------+------------+------------+--------------+---------------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Weather Condition Grouping\n\nWe simplify the `Weather_Condition` column by mapping all detailed weather descriptions to broader, more meaningful groups (e.g., \"Rainy\", \"Snowy\", \"Cloudy\") to focus on high-level weather patterns in our analysis.","metadata":{}},{"cell_type":"code","source":"# Drop rows where Weather_Condition is 'N/A Precipitation' (only 1000 rows)\nif \"Weather_Condition\" in df.columns:\n    df = df.filter(col(\"Weather_Condition\") != \"N/A Precipitation\")\n\n# Define mapping from detailed weather conditions to broader groups\nweather_groups = {\n    \"Clear\": [\"Clear\", \"Fair\"],\n\n    \"Cloudy\": [\n        \"Partly Cloudy\", \"Mostly Cloudy\", \"Cloudy\", \"Overcast\", \"Scattered Clouds\"\n    ],\n\n    \"Hazy & Dusty\": [\n        \"Fog\", \"Shallow Fog\", \"Patches of Fog\", \"Partial Fog\", \"Light Fog\", \"Mist\",\n        \"Light Haze\", \"Haze\", \"Smoke\",\n        \"Freezing Fog\", \"Light Freezing Fog\",\n        \"Widespread Dust\", \"Blowing Dust\", \"Sand / Dust Whirls Nearby\", \"Sand / Dust Whirlwinds\"\n    ],\n\n    \"Rainy\": [\n        \"Rain\", \"Light Rain\", \"Heavy Rain\", \"Rain Showers\", \"Rain Shower\", \n        \"Light Rain Showers\", \"Heavy Rain Showers\", \"Showers in the Vicinity\",\n        \"Drizzle\", \"Light Drizzle\", \"Heavy Drizzle\", \"Heavy Rain Shower\", \"Light Rain Shower\", \"Squalls\"\n    ],\n\n    \"Thunderstorm or Hail\": [\n        \"Thunderstorm\", \"Thunder\", \"T-Storm\", \"Heavy T-Storm\", \"Thunder in the Vicinity\", \n        \"Thunderstorms and Rain\", \"Light Thunderstorms and Rain\", \"Heavy Thunderstorms and Rain\",\n        \"Thunder and Hail\", \"Light Thunderstorm\", \"Light Rain with Thunder\",\n        \"Heavy Thunderstorms with Small Hail\", \"Thunder / Wintry Mix\",\n        \"Snow and Thunder\", \"Light Snow with Thunder\", \"Light Thunderstorms and Snow\",\n        \"Funnel Cloud\", \"Tornado\",\n        \"Hail\", \"Small Hail\", \"Light Hail\"\n    ],\n\n    \"Snowy\": [\n        \"Snow\", \"Light Snow\", \"Heavy Snow\", \"Snow Showers\", \"Light Snow Showers\",\n        \"Heavy Snow Shower\", \"Light Snow Shower\", \"Drifting Snow\", \"Blowing Snow\",\n        \"Light Blowing Snow\", \"Snow Grains\", \"Light Snow Grains\", \"Wintry Mix\",\n        \"Snow and Sleet\", \"Light Snow and Sleet\"\n    ],\n\n    \"Freezing Rain & Ice\": [\n        \"Freezing Rain\", \"Light Freezing Rain\", \"Heavy Freezing Rain\",\n        \"Freezing Drizzle\", \"Light Freezing Drizzle\", \"Heavy Freezing Drizzle\",\n        \"Ice Pellets\", \"Light Ice Pellets\", \"Heavy Ice Pellets\",\n        \"Sleet\", \"Light Sleet\", \"Heavy Sleet\", \"Rain and Sleet\"\n    ]\n\n}\n\n# Invert mapping for fast lookup\nweather_map = {}\nfor group, items in weather_groups.items():\n    for item in items:\n        weather_map[item] = group\n\n# Keep only rows with Weather_Condition in the mapping\nif \"Weather_Condition\" in df.columns:\n    mapped_conditions = list(weather_map.keys())\n    df = df.filter(col(\"Weather_Condition\").isin(mapped_conditions))\n\ndef map_weather(cond):\n    if cond is None:\n        return \"Unknown\"\n    return weather_map[cond]  # No default, since all values are mapped\n\nweather_udf = udf(map_weather, StringType())\nif \"Weather_Condition\" in df.columns:\n    df = df.withColumn(\"Weather_Condition\", weather_udf(df[\"Weather_Condition\"]))\n    print(\"Unique values in 'Weather_Condition' after grouping:\")\n    df.select(\"Weather_Condition\").distinct().show(truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:08:24.679275Z","iopub.execute_input":"2025-04-27T23:08:24.679690Z","iopub.status.idle":"2025-04-27T23:08:47.128782Z","shell.execute_reply.started":"2025-04-27T23:08:24.679654Z","shell.execute_reply":"2025-04-27T23:08:47.126578Z"}},"outputs":[{"name":"stdout","text":"Unique values in 'Weather_Condition' after grouping:\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"+--------------------+\n|Weather_Condition   |\n+--------------------+\n|Cloudy              |\n|Snowy               |\n|Thunderstorm or Hail|\n|Clear               |\n|Rainy               |\n|Freezing Rain & Ice |\n|Hazy & Dusty        |\n+--------------------+\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Urban vs. Rural Classification\n\nWe enrich our dataset by classifying each accident as occurring in an urban or rural area. We do this by joining with a list of US urban cities, matching on city and state. If a match is found, the accident is labeled \"Urban\"; otherwise, it is labeled \"Rural\".","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import concat_ws, trim, upper, col, when\n\n# Load the US urban cities dataset\ncities_path = \"/kaggle/input/us-urban-cities/US Urban Cities.csv\"\ndf_cities = spark.read.csv(cities_path, header=True, inferSchema=True)\n\n# Prepare city-state keys for joining (ensure function names are not overwritten)\ndf = df.withColumn(\"city_state_key\", concat_ws(\", \", trim(col(\"City\")), trim(col(\"State\"))))\ndf_cities = df_cities.withColumn(\"city_state_key\", upper(trim(col(\"NAME\"))))\n\n# Also uppercase the accident key for robust matching\ndf = df.withColumn(\"city_state_key\", upper(col(\"city_state_key\")))\n\n# Join to determine Urban/Rural\ndf = df.join(\n    df_cities.select(\"city_state_key\").withColumn(\"Urban_Rural\", when(col(\"city_state_key\").isNotNull(), \"Urban\")),\n    on=\"city_state_key\",\n    how=\"left\"\n)\n\n# Fill nulls with \"Rural\"\ndf = df.withColumn(\"Urban_Rural\", when(col(\"Urban_Rural\").isNull(), \"Rural\").otherwise(col(\"Urban_Rural\")))\n\n# Drop the helper key column if you want\ndf = df.drop(\"city_state_key\")\n\n# Check the result: show counts for Urban and Rural\ndf.groupBy(\"Urban_Rural\").count().show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:08:47.129571Z","iopub.execute_input":"2025-04-27T23:08:47.129870Z","iopub.status.idle":"2025-04-27T23:09:00.007752Z","shell.execute_reply.started":"2025-04-27T23:08:47.129846Z","shell.execute_reply":"2025-04-27T23:09:00.004917Z"}},"outputs":[{"name":"stderr","text":"[Stage 94:========================================================================>   (22 + 1) / 23]\r","output_type":"stream"},{"name":"stdout","text":"+-----------+-------+\n|Urban_Rural|  count|\n+-----------+-------+\n|      Urban|1884199|\n|      Rural|4255990|\n+-----------+-------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Final Dataset: Before vs. After Preprocessing\n\n- **Start:** 7,728,394 rows, 46 columns (with missing values, redundant, and raw features)\n- **End:** 6,140,189 rows, 42 columns (fully cleaned, binned, grouped, and enriched)\n\n**Key changes:**  \n- Removed columns with high missing or low analytical value  \n- Filtered outliers and binned continuous variables  \n- Grouped weather conditions  \n- Added Urban/Rural classification","metadata":{}},{"cell_type":"code","source":"print(f\"Rows: {df.count()}, Columns: {len(df.columns)}\")\ndf.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:09:00.008592Z","iopub.execute_input":"2025-04-27T23:09:00.009485Z","iopub.status.idle":"2025-04-27T23:09:11.784950Z","shell.execute_reply.started":"2025-04-27T23:09:00.009450Z","shell.execute_reply":"2025-04-27T23:09:11.784093Z"}},"outputs":[{"name":"stderr","text":"[Stage 98:=====================================================================>      (21 + 2) / 23]\r","output_type":"stream"},{"name":"stdout","text":"Rows: 6140189, Columns: 42\nroot\n |-- Severity: integer (nullable = true)\n |-- Start_Time: timestamp (nullable = true)\n |-- End_Time: timestamp (nullable = true)\n |-- Start_Lat: double (nullable = true)\n |-- Start_Lng: double (nullable = true)\n |-- Distance(mi)_cont: double (nullable = true)\n |-- Description: string (nullable = true)\n |-- Street: string (nullable = true)\n |-- City: string (nullable = true)\n |-- County: string (nullable = true)\n |-- State: string (nullable = true)\n |-- Zipcode: string (nullable = true)\n |-- Timezone: string (nullable = true)\n |-- Airport_Code: string (nullable = true)\n |-- Weather_Timestamp: timestamp (nullable = true)\n |-- Humidity(%)_cont: double (nullable = true)\n |-- Pressure(in)_cont: double (nullable = true)\n |-- Visibility(mi)_cont: double (nullable = true)\n |-- Wind_Direction: string (nullable = true)\n |-- Wind_Speed(mph)_cont: double (nullable = true)\n |-- Weather_Condition: string (nullable = true)\n |-- Amenity: boolean (nullable = true)\n |-- Bump: boolean (nullable = true)\n |-- Crossing: boolean (nullable = true)\n |-- Give_Way: boolean (nullable = true)\n |-- Junction: boolean (nullable = true)\n |-- No_Exit: boolean (nullable = true)\n |-- Railway: boolean (nullable = true)\n |-- Roundabout: boolean (nullable = true)\n |-- Station: boolean (nullable = true)\n |-- Stop: boolean (nullable = true)\n |-- Traffic_Calming: boolean (nullable = true)\n |-- Traffic_Signal: boolean (nullable = true)\n |-- Sunrise_Sunset: string (nullable = true)\n |-- Distance(mi): string (nullable = false)\n |-- Temperature(C)_cont: double (nullable = true)\n |-- Temperature(C): string (nullable = false)\n |-- Humidity(%): string (nullable = false)\n |-- Pressure(in): string (nullable = false)\n |-- Visibility(mi): string (nullable = false)\n |-- Wind_Speed(mph): string (nullable = false)\n |-- Urban_Rural: string (nullable = true)\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Exporting the Cleaned Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\n# Write to a directory (not a file)\noutput_dir = \"/kaggle/working/US_Accidents_Cleaned\"\ndf.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_dir)\n\nprint(f\"Cleaned dataset exported as a directory to: {output_dir}\")\n\n# Stop Spark session\nspark.stop()\n\n# Find the part file and move it\npart_file = [f for f in os.listdir(output_dir) if f.startswith(\"part-\") and f.endswith(\".csv\")][0]\nshutil.move(os.path.join(output_dir, part_file), \"/kaggle/working/US_Accidents_Cleaned.csv\")\nshutil.rmtree(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:09:11.785727Z","iopub.execute_input":"2025-04-27T23:09:11.786042Z","iopub.status.idle":"2025-04-27T23:11:15.553849Z","shell.execute_reply.started":"2025-04-27T23:09:11.786016Z","shell.execute_reply":"2025-04-27T23:11:15.552345Z"}},"outputs":[{"name":"stderr","text":"                                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Cleaned dataset exported as a directory to: /kaggle/working/US_Accidents_Cleaned\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Reading Exported Dataset in Pandas","metadata":{}},{"cell_type":"code","source":"df_pd = pd.read_csv(\"/kaggle/working/US_Accidents_Cleaned.csv\")\nprint(df_pd.info())\nprint(df_pd.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T23:11:15.556061Z","iopub.execute_input":"2025-04-27T23:11:15.557289Z","iopub.status.idle":"2025-04-27T23:13:04.893747Z","shell.execute_reply.started":"2025-04-27T23:11:15.557258Z","shell.execute_reply":"2025-04-27T23:13:04.892705Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6140189 entries, 0 to 6140188\nData columns (total 42 columns):\n #   Column                Dtype  \n---  ------                -----  \n 0   Severity              int64  \n 1   Start_Time            object \n 2   End_Time              object \n 3   Start_Lat             float64\n 4   Start_Lng             float64\n 5   Distance(mi)_cont     float64\n 6   Description           object \n 7   Street                object \n 8   City                  object \n 9   County                object \n 10  State                 object \n 11  Zipcode               object \n 12  Timezone              object \n 13  Airport_Code          object \n 14  Weather_Timestamp     object \n 15  Humidity(%)_cont      float64\n 16  Pressure(in)_cont     float64\n 17  Visibility(mi)_cont   float64\n 18  Wind_Direction        object \n 19  Wind_Speed(mph)_cont  float64\n 20  Weather_Condition     object \n 21  Amenity               bool   \n 22  Bump                  bool   \n 23  Crossing              bool   \n 24  Give_Way              bool   \n 25  Junction              bool   \n 26  No_Exit               bool   \n 27  Railway               bool   \n 28  Roundabout            bool   \n 29  Station               bool   \n 30  Stop                  bool   \n 31  Traffic_Calming       bool   \n 32  Traffic_Signal        bool   \n 33  Sunrise_Sunset        object \n 34  Distance(mi)          object \n 35  Temperature(C)_cont   float64\n 36  Temperature(C)        object \n 37  Humidity(%)           object \n 38  Pressure(in)          object \n 39  Visibility(mi)        object \n 40  Wind_Speed(mph)       object \n 41  Urban_Rural           object \ndtypes: bool(12), float64(8), int64(1), object(21)\nmemory usage: 1.4+ GB\nNone\n   Severity                Start_Time                  End_Time  Start_Lat  \\\n0         2  2016-02-08T06:49:27.000Z  2016-02-08T07:19:27.000Z  39.063148   \n1         3  2016-02-08T07:23:34.000Z  2016-02-08T07:53:34.000Z  39.747753   \n2         2  2016-02-08T07:39:07.000Z  2016-02-08T08:09:07.000Z  39.627781   \n3         3  2016-02-08T07:44:26.000Z  2016-02-08T08:14:26.000Z  40.100590   \n4         2  2016-02-08T07:59:35.000Z  2016-02-08T08:29:35.000Z  39.758274   \n\n   Start_Lng  Distance(mi)_cont  \\\n0 -84.032608               0.01   \n1 -84.205582               0.01   \n2 -84.188354               0.01   \n3 -82.925194               0.01   \n4 -84.230507               0.00   \n\n                                         Description  \\\n0  Accident on OH-32 State Route 32 Westbound at ...   \n1  Accident on I-75 Southbound at Exits 52 52B US...   \n2  Accident on McEwen Rd at OH-725 Miamisburg Cen...   \n3  Accident on I-270 Outerbelt Northbound near Ex...   \n4  Accident on Oakridge Dr at Woodward Ave. Expec...   \n\n                      Street          City      County  ... Traffic_Signal  \\\n0             State Route 32  Williamsburg    Clermont  ...           True   \n1                     I-75 S        Dayton  Montgomery  ...          False   \n2  Miamisburg Centerville Rd        Dayton  Montgomery  ...           True   \n3             Westerville Rd   Westerville    Franklin  ...          False   \n4             N Woodward Ave        Dayton  Montgomery  ...          False   \n\n  Sunrise_Sunset Distance(mi) Temperature(C)_cont Temperature(C)  \\\n0          Night    0.00â€“1.50            2.222222     2.22â€“12.78   \n1          Night    0.00â€“1.50            1.722222     -8.33â€“2.22   \n2            Day    0.00â€“1.50            2.222222     2.22â€“12.78   \n3            Day    0.00â€“1.50            3.277778     2.22â€“12.78   \n4            Day    0.00â€“1.50            1.111111     -8.33â€“2.22   \n\n    Humidity(%)  Pressure(in)  Visibility(mi) Wind_Speed(mph)  Urban_Rural  \n0  79.25â€“100.00   28.72â€“30.35      7.00â€“10.00       0.00â€“5.00        Rural  \n1  79.25â€“100.00   28.72â€“30.35      7.00â€“10.00       0.00â€“5.00        Urban  \n2  79.25â€“100.00   28.72â€“30.35       4.00â€“7.00       0.00â€“5.00        Urban  \n3  79.25â€“100.00   28.72â€“30.35      7.00â€“10.00       0.00â€“5.00        Rural  \n4  79.25â€“100.00   28.72â€“30.35      7.00â€“10.00       0.00â€“5.00        Urban  \n\n[5 rows x 42 columns]\n","output_type":"stream"}],"execution_count":14}]}